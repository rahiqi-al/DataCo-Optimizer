version: '3'

services:
  postgres:
    image: postgres:13
    container_name: postgres
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    restart: unless-stopped

  airflow:
    image: apache/airflow:2.8.1
    container_name: airflow
    user: "0"
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - PYTHONPATH=/project
    ports:
      - "8081:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # Add this
      - ./dags:/opt/airflow/dags
      - .:/project
      - ./logs:/opt/airflow/logs
    command: bash -c "sleep 60; while ! nc -z postgres 5432; do sleep 1; done; airflow db init && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin && airflow webserver & airflow scheduler"
    restart: unless-stopped

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2182:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    restart: unless-stopped

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: mlflow
    depends_on:
      - minio
    ports:
      - "5000:5000"
    volumes:
      - ./scripts/ml_model.py:/mlflow/scripts/ml_model.py
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=ali
      - AWS_SECRET_ACCESS_KEY=aliali123
      - MLFLOW_ARTIFACTS_DESTINATION=s3://mlflow-artifacts
    command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:////mlflow/mlflow.db --artifacts-destination s3://mlflow-artifacts
    restart: unless-stopped

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    environment:
      - MINIO_ROOT_USER=ali
      - MINIO_ROOT_PASSWORD=aliali123
    command: server /data --console-address ":9001"
    restart: unless-stopped

  dbt:
    image: ghcr.io/dbt-labs/dbt-snowflake:latest
    container_name: dbt
    volumes:
      - ./dbt_project:/usr/app/dbt
    environment:
      - SNOWFLAKE_ACCOUNT=xbmgybc-in88930
      - SNOWFLAKE_USER=ali
      - SNOWFLAKE_PASSWORD=xSPJ8viZmZfjMxD
      - SNOWFLAKE_ROLE=ACCOUNTADMIN
      - SNOWFLAKE_DATABASE=dataco
      - SNOWFLAKE_WAREHOUSE=dataco
      - SNOWFLAKE_SCHEMA=public
    working_dir: /usr/app/dbt
    entrypoint: ["/bin/bash", "-c"]
    command: ["sleep infinity"]  # Keeps container idle
    restart: no

volumes:
  postgres_data:
  zookeeper_data:
  kafka_data:
  minio_data: